{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protective-grammar",
   "metadata": {},
   "source": [
    "# Train a LSTM on sine wave data\n",
    "\n",
    "Let's explain the code below. \n",
    "\n",
    "First we import our usual libraries. To generate the sine wave training data, we generate random integers and pass them through `np.sin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#set seed to be able to replicate the resutls\n",
    "seed = 172\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def generate_sin_wave_data():\n",
    "    T = 20\n",
    "    L = 1000\n",
    "    N = 200\n",
    "\n",
    "    x = np.empty((N, L), 'int64')\n",
    "    x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "    data = np.sin(x / 1.0 / T).astype('float64')\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-divide",
   "metadata": {},
   "source": [
    "The model consists of two LSTM cells as we already mentioned. The first cell receives an input of length 1 and has an output of length 51 while the second one receives an input of length 51 and has an output of length 1.\n",
    "\n",
    "Have a closer look in the `forward` method. Did you noticed that we can generate future predictions? The first for-loop runs on all data points in the input data. The second for loop recieves thet last data point and tries to generate new ones for the next time step.\n",
    "\n",
    "For each training epoch, we train the model and then we generate 1000 new data points.\n",
    "\n",
    "After each epoch, we plot the predicted data points to visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.LSTMCell(1, 51)\n",
    "        self.rnn2 = nn.LSTMCell(51, 51)\n",
    "\n",
    "        self.linear = nn.Linear(51, 1)\n",
    "\n",
    "    def forward(self, input, future=0):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "\n",
    "            h_t, c_t = self.rnn1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.rnn2(h_t, (h_t2, c_t2))\n",
    "\n",
    "\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "\n",
    "        # if we should predict the future\n",
    "        for i in range(future):\n",
    "\n",
    "            h_t, c_t = self.rnn1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.rnn2(h_t, (h_t2, c_t2))\n",
    "\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def train():\n",
    "    # load data and make training set\n",
    "    data = generate_sin_wave_data()\n",
    "    input = torch.from_numpy(data[3:, :-1])\n",
    "    target = torch.from_numpy(data[3:, 1:])\n",
    "    test_input = torch.from_numpy(data[:3, :-1])\n",
    "    test_target = torch.from_numpy(data[:3, 1:])\n",
    "\n",
    "    seq = Sequence()\n",
    "\n",
    "    seq.double()\n",
    "    criterion = nn.MSELoss()\n",
    "    # use LBFGS as optimizer since we can load the whole data to train\n",
    "    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
    "    \n",
    "    # begin to train\n",
    "    for i in range(1):\n",
    "        print('STEP: ', i)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input)\n",
    "            loss = criterion(out, target)\n",
    "            print('loss:', loss.item())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "        # begin to predict, no need to track gradient here\n",
    "        with torch.no_grad():\n",
    "            future = 1000\n",
    "            pred = seq(test_input, future=future)\n",
    "            loss = criterion(pred[:, :-future], test_target)\n",
    "            print('test loss:', loss.item())\n",
    "            y = pred.detach().numpy()\n",
    "            \n",
    "        # draw the result\n",
    "        plt.figure(figsize=(30, 10))\n",
    "        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
    "        plt.xlabel('x', fontsize=20)\n",
    "        plt.ylabel('y', fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "\n",
    "        def draw(yi, color):\n",
    "            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth=2.0)\n",
    "            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth=2.0)\n",
    "\n",
    "        draw(y[0], 'r')\n",
    "        draw(y[1], 'g')\n",
    "        draw(y[2], 'b')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generate_sin_wave_data()\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-authorization",
   "metadata": {},
   "source": [
    "Now I have a few more tasks for you. Of course, they are optional.\n",
    "\n",
    "- Try to add more layers to the network and see if the model fits\n",
    "- Play around with the hyperparameters and compare the results\n",
    "- Try to generate cosine wave data and train the network on them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
